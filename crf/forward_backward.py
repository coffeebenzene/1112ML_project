import numpy as np
import itertools

# forward-backward algorithm is also known as sum_product algorithm.

def potential_func(w, vfeature_func, sentence):
    """w: numpy array of weights for the features
       vfeature_func: Vectorised feature function generated by crf_feature.make_vfeature_func
       sentence: List of words.
       
       sentence is assumed to have low-frequency words already substituted for #UNK#
       
       returns a function that calculates:
       potential: e^(weight . features(input))
    """
    def potential(yp, y, i): # Potential of state yp to y at index i.
        features = vfeature_func(yp, y, i, sentence)
        return np.exp(np.dot(w, features))
    
    return potential

def forward_backward(potential, sentence, states):
    """potential: Potential function e^(weight . features)
       sentence: List of words.
       state: list of states/tags. Don't include START/STOP.
       
       sentence is assumed to have low-frequency words already substituted for #UNK#
       
       Returns 2D numpy array of [[state1_score, state2_score...], ...]
           # row = index, col = state.
       ----------IGNORE BELOW----------
       Returns 2 list of dicts of [{state: score, ...}, ...]
           #Each index of the list corresponds to index in the column (index 0 is for word 0)
           #Each dict is {state/tag: score} for that index.
           #i.e. row = index, col = state.
       First list is the alpha/forward scores.
       Second list is the beta/backward scores.
       
       NOTE: It is possible to change the list of dicts to a numpy 2D-array. It may be faster, but this is not done. Ease of coding first.
    """
    
    # Calculate alpha scores
    alpha_table = []
    # Initial forward step.
    d = {}
    for u in states:
        d[u] = potential("START", u, 0)
    alpha_table.append(d)
    # subsequent forward steps
    for i in range(1,len(sentence)):
        d = {}
        for u in states:
            d[u] = sum( alpha_table[i-1][v]*potential(v, u, i) for v in states )
        alpha_table.append(d)
    
    # Calculate beta scores
    beta_table = [None for word in sentence] # preinitialize length
    # Initial backward step.
    d = {}
    for u in states:
        d[u] = potential(u, "END", None)
    beta_table[-1] = d
    # subsequent backward steps
    for i in range(len(sentence)-2, -1, -1):
        d = {}
        for u in states:
            d[u] = sum( beta_table[i+1][v]*potential(u, v, i+1) for v in states )
        beta_table[i] = d
    
    return alpha_table, beta_table



def calc_z_marginals(alpha_table, beta_table, potential, states):
    """Calculate:
       1. normalization constant Z = sum of all potentials.
       2. marginal probability P(y_{i-1}=u, y_i=v |x) for each value of i, u and v.
          (except at START/STOP edges, i.e. from i=0 to len(sentence)-1)
       
       returns:
        1. Scalar value z
        2. a dict of { (i,u,v): probability }
    """
    # Any index should work
    last_a = alpha_table[-1]
    last_b = beta_table[-1]
    
    # Calculate normalization constant
    z = 0
    for state in states: # note: np 2D array can just multiply element wise and sum.
        a_score = last_a[state]
        b_score = last_b[state]
        z += a_score*b_score
    
    # Calculate marginals
    marginals = {}
    for i in range(len(alpha_table)-1):
        for u,v in itertools.product(states, states):
            marginals[(i,u,v)] = alpha_table[i][u]*potential(u,v,i+1)*beta_table[i][v]
    i = len(alpha_table)-1
    for u,v in itertools.product(states, states):
        marginals[(i,u,v)] = alpha_table[i][u]*potential(u,v,None)
    
    return z, marginals